Graph500 Implementation - Nanyang Technological University
====================================================================

--------------------------------------------------------------------
Location of Sources Code
--------------------------------------------------------------------
Our source code goes in the graph500-2.1.4/mpi/src


--------------------------------------------------------------------
What is Graph500?
--------------------------------------------------------------------
The Graph500 is a rating of supercomputer systems, focused on Data intensive loads. The intent of this Graph500 benchmark problem ("Search") is to develop a compact application that has multiple analysis techniques (multiple kernels) accessing a single data structure representing a weighted, undirected graph. In addition to a kernel to construct the graph from the input tuple list, there is one additional computational kernel to operate on the graph. This benchmark includes a scalable data generator which produces edge tuples containing the start vertex and end vertex for each edge. 
- The first kernel constructs an undirected graph in a format usable by all subsequent kernels. No subsequent modifications are permitted to benefit specific kernels. 
- The second kernel performs a breadth-first search of the graph. 

www.graph500.org


--------------------------------------------------------------------
Our Understanding of Graph500
--------------------------------------------------------------------
The problem of Graph500 is a BFS in a sparse undirected graph. The computation of BFS is light, and the bottleneck of the problem is at memory speed. As a result, the scalability of the algorithm for Graph500 will be determined by how the algorithm utilizing the memory reading. In addition, to manager cluster to run the algorithm, the communication between cluster nodes is also an important factor.

To conquer the problem, we also need to understand the characteristics of the input graph and make optimization based on those characteristics.

--------------------------------------------------------------------
Our Implementation
--------------------------------------------------------------------
1. 
At top level, we use level synchronized BFS, which means under the context of multiple nodes, we will sync the frontier among nodes at every level. Level synchronized BFS makes us easier to do further optimize.

All nodes share a same frontier. At each level, each node will find vertex for next level by looking the vertex in frontier (current level). After each node finds which vertex will be in next level, they will sync those nodes to make the new frontier. 

The algorithm ends when there is no vertex in the frontier.

2.
We use 1-D paritition for our graph. The reason is our cluster is small and the communication cost is little. So we may not need a 2-D partition on our graph to reduce communication cost.

2.
We combine top-down BFS and bottom-up BFS. Top-down BFS is the standard BFS we are commonly using. Bottom-up BFS means each unvisited vertex searches its neighbor to find its parent. This choice is based on the characteristics of the graph. During BFS, the frontier usually expands exponentially and reach the peak in few levels. When the number of vertex in frontier is large, the bottom-up BFS will run faster than top-down BFS.

3. 
In our algorithm, the only part that needs communication between nodes is syncing the frontier. Simply syncing the index of vertex among nodes will cause huge network traffic. Since we only need to know which vertex is in the frontier or not, we can use bitmap to represent the frontier. As a result, we reduce the traffic significantly.

4.
Looking into the input graph, we found that the graph contains a lot of duplicated edges and zero-degree vertex. Getting rid of those will be beneficial to the performance. As a result, at graph-building stage, we also filter out the duplicated edges and zero-degree vertex. This gives us around 2x~3x speedup.

5.
We explored the implementation between CPU and GPU. In the GPU implementation, each thread of GPU in charges of one edge. The result shows the CPU has better performance than GPU.

--------------------------------------------------------------------
Running
--------------------------------------------------------------------
Our cluster has 3 nodes with 6 GPU. So we run our algorithm in 2 nodes. In each node, we run 2 MPI process, so totally 4 MPI processes in 2 nodes. Each MPI process use 12 threads to fully occupy the node.

--------------------------------------------------------------------
Looking into Future
--------------------------------------------------------------------
1.
During the profiling, we found that at scale > 20, our algorithm is affected by the cache efficiency. The profile tool Valgrind tells us our algorithm has around 10% data cache miss. Thus, we need to solve the cache miss problem.

2.
We need to explore more about the parallel BFS techniques and apply to our algorithm. For example, one thing is we need to use 2-D partition if we need to scale into a large cluster.
